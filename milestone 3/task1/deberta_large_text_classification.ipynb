{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "t3IxC6FH3HC4"
      },
      "source": [
        "#DeBERTa large"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZQaWfFsFaNT",
        "outputId": "a3f6aefd-3dc0-4fcc-b003-baaaf1dc7a0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting evaluate\n",
            "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0 (from transformers)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.7,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.4.0)\n",
            "Collecting aiohttp (from datasets)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting responses<0.19 (from datasets)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: tokenizers, xxhash, multidict, frozenlist, dill, async-timeout, yarl, responses, multiprocess, huggingface-hub, aiosignal, transformers, aiohttp, datasets, evaluate\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.12.0 dill-0.3.6 evaluate-0.4.0 frozenlist-1.3.3 huggingface-hub-0.14.1 multidict-6.0.4 multiprocess-0.70.14 responses-0.18.0 tokenizers-0.13.3 transformers-4.28.1 xxhash-3.2.0 yarl-1.9.2\n"
          ]
        }
      ],
      "source": [
        "#install transformers datasets evaluate\n",
        "!pip install transformers datasets evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87,
          "referenced_widgets": [
            "852d8f7b7e314e38b19729752dd9809a",
            "7ef2bc4eafd14d5fbda17f8d7afe122d",
            "a99191ae496e499f8612fac657d384c2",
            "35b0aa0387a14fe4b00dfc59447af139",
            "a3f35a23095140a0a8b473e3190e1ca3",
            "e6693bd77d694161962a0343ebdc0f40",
            "8ea4256e75a54222a279174294893f6d",
            "cdeee073d5ca40db8a3eb72b322d5a23",
            "0868f02c2ec3436aa196a09623b08655",
            "78c297ac70f84334b2e8ad022ac90552",
            "4d6c1d93b1404e5bb0227d12ca9d4cc0"
          ]
        },
        "id": "oqb9X5cE3HC5",
        "outputId": "476d1ce8-9e6b-4474-f030-93b3a36314fa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:datasets.builder:Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-a39e475095145fe4/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "852d8f7b7e314e38b19729752dd9809a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#import the dataset\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset('csv', data_files={'train': 'df_train.csv',\n",
        "                                          'val': 'df_valid.csv',\n",
        "                                              'test': 'df_test.csv'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "da4d5af346cc4459a84e9bc8bf358b8d",
            "bd5605b324584487ad4f159f9145f548",
            "905e09e0188644bd99e0df60cee13456",
            "e90a378a80a84f69911af83a117aaff1",
            "874895fc52f740cb9b2ef484261783e1",
            "6f1ad411120743df826ec99ba39be086",
            "19090a3501d4441a8ce27a133a106b47",
            "41e90beefd4e48a9814e69018eaa3929",
            "3d827f60bec244de816cdd9475f3b541",
            "a05acb4ddb294641aef3a275bfbf0742",
            "7bb5acfe8ab84bc0b741d1b94c1a62b1",
            "cedf2bf130bc433389e43b9a927fa6f4",
            "687f5baa05bd4fc3942e86b64c65daf8",
            "5157b5aa411349538d2d2832b1980dee",
            "1e5ee4a64ad74da595e054c0fa14cb78",
            "4830d1c2d11e4f29891208e039034117",
            "10febeca6c3141cca781055685c36025",
            "f7d6583a51154b298f72825634268ef9",
            "ef1ea8aa68c242edb0aef4a9b21d0887",
            "dd35b7e9ef2d4d2d873a623f76ea027a",
            "95a7f60f08e64fbb84ce6e9e19df3434",
            "f18beb662cb94dba8a3c5a8d417f4994",
            "422d3870c334407a93ff3d9b2d0a9d48",
            "ef6c59e39bea43b29f54a8d5c571e7f7",
            "946034c51cb346cda431a1e7311ef6c5",
            "351f1937fb9643fa9083599af99b4c7f",
            "d20a7351a26f4cbf91bd3b0ddf59d926",
            "4af1e9919235476db92366f572956df2",
            "5a8a7a6ff1474cfcbd8ffcb0fe603cbb",
            "6a05e552faa14b77affd5b3220867d38",
            "1dae37860d9b49da90f616eb6dcf0ef8",
            "a19212a57b764a828feb6dcc1cedd5cf",
            "29b38d2459224bda88e747a5f98592a3",
            "7e997b0464dc4c499067aa93a38c463a",
            "bd79b23445cc499ca1e13ea472a17410",
            "aadf5c98c81f4651b1f8db72ce98f065",
            "14ab4851224447b39f5bcf33fdecbfda",
            "e1690d212372471186b2831a3bd03d62",
            "6bbca0e087964c2b80952b6eaa9b500c",
            "97901b522efa4641ac052905c5cfa989",
            "261f6dcbc952423c999107e58a4cb98f",
            "2ec5ab6476fa48a89ea0b79a381af2f0",
            "fcea6663406e4cda9a1fdc1e598103f4",
            "7c7f74dd17b148afa805b7ba1a308c5f"
          ]
        },
        "id": "gVzLy82d7KE3",
        "outputId": "2f49b4a7-d01c-40ab-b8d8-975105727b45"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "da4d5af346cc4459a84e9bc8bf358b8d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cedf2bf130bc433389e43b9a927fa6f4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/475 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "422d3870c334407a93ff3d9b2d0a9d48",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7e997b0464dc4c499067aa93a38c463a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-large\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oV5ri_4l3HC5"
      },
      "outputs": [],
      "source": [
        "#preprocess the data by tokenizing the text\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples['text'], padding='max_length',\n",
        "                                max_length=128,\n",
        "                                truncation=True,\n",
        "                                return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVG-xFpD3HC5"
      },
      "outputs": [],
      "source": [
        "#declare dictionaries to map the labels\n",
        "id2label = {0: \"phrase\", 1: \"passage\", 2: \"multi\"}\n",
        "label2id = {\"phrase\": 0, \"passage\": 1, \"multi\": 2}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158,
          "referenced_widgets": [
            "28145011e9d64625a3685f22947a11e5",
            "73e775ec05594f95899ba49180fa5149",
            "3c199862837546188290a3209c93766a",
            "927162f4c809490f8bd7c0c686ee0c42",
            "7e270a0601e7430cac64118bd5c73c05",
            "e15c3f954f1d4aee9ef1311fabe48c92",
            "7be6b287574f4bef812dcd0e8898e9e0",
            "1d35d6c1273a4b119d1746557d8f5ac2",
            "d636598798184af9a10145c677169fd8",
            "f4c8966dd89146568543c534835c9290",
            "30f04010bf4c49b294adca75bd4e18f7"
          ]
        },
        "id": "F76tUn5F7KE3",
        "outputId": "cdad05f1-d1fe-4770-cf4e-ee2caf9d750e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "28145011e9d64625a3685f22947a11e5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at microsoft/deberta-large were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias']\n",
            "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-large and are newly initialized: ['classifier.bias', 'pooler.dense.bias', 'classifier.weight', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"microsoft/deberta-large\", num_labels=3, id2label=id2label, label2id=label2id\n",
        ").to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "2423e80c3fe949aa80da764e59589797",
            "4f42dab2b1cd4ba8be5b46f442400ebb",
            "0989c8806a7d4f4ba246c030e3e2837c",
            "f9641d12c5cc435198d0502a5448ac81",
            "6fd2bafda1464539962d17f847a59787",
            "11567ac6d0a7447a98ba85e8bb1ca4c3",
            "2252164ac80347ea92206ec16d6e719c",
            "281f5f4852114d64be6bdfe604b33177",
            "40a68232196e4f2fb0c06f5e0785ba49",
            "16d36e7e4d4d4391841a8859ae08325f",
            "8c4e9a2d637f4b86addb50e9487f3e05",
            "8fd327937a4a4c1e9631e615b26f4817",
            "811f1956f1fd4903a95396586d135f14",
            "18080b31c3ea4aa3b0bb4bc657691bdc",
            "0ba9b3338dbb4bf6bd50325b3132b36f",
            "604ed017db48485390c22abcbeb7c377",
            "21d88b53a6d14de3add79f430be6f661",
            "6ddb5d31a1ae4d139e234523763d9a2e",
            "0ad9911d5c8645dfa3b5ad8183c94d98",
            "cb5d1a66a7ec439abb849fd994391bbd",
            "c3a2d7c0a8c54ee4a0ff9febcddd1838",
            "4d9cf52757b24f06a04997ef6261ec4c",
            "3046c933096b45928649d3448929cc1b",
            "b51c008228614ecc93b461a4e3d09356",
            "e4b7acf28ef5493fb3b4fbbcfb190978",
            "461d618a4a5b46e691e728f1c82e894f",
            "2066eaabe4934baeb5be491c50a7703b",
            "090dde3c1a81426197b38a2ca1c79a66",
            "f888f1e175af4e0abbf582f82e6e7b89",
            "4a85f2fae84b4e7392c95f14727b76dd",
            "a5ce25fb627041a9bc30fc4763b3a6cb",
            "fbbd68c56b0345edbc5b5b158dc71803",
            "1bb263c53e3541d5a460cc52c4fc3eb5"
          ]
        },
        "id": "eYCA2Cp23HC5",
        "outputId": "42aff363-6fa5-449c-eadf-cb7d896077e4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2423e80c3fe949aa80da764e59589797",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/2560 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8fd327937a4a4c1e9631e615b26f4817",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/640 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3046c933096b45928649d3448929cc1b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#run the preprocess function on the input text\n",
        "tokenized_data = dataset.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "um4PgnHZ3HC5"
      },
      "outputs": [],
      "source": [
        "#import the metrics\n",
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "accuracy_metric = evaluate.load(\"accuracy\")\n",
        "f1_metric = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "\n",
        "  logits, labels = eval_pred\n",
        "  predictions = np.argmax(logits, axis=-1)\n",
        "  f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"macro\")[\"f1\"]\n",
        "  accuracy = accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
        "\n",
        "  return {\"f1\": f1, \"accuracy\": accuracy}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHKLu4QN3HC6"
      },
      "outputs": [],
      "source": [
        "#import the data collator of huggingface\n",
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jE6o5Xjq3HC6"
      },
      "source": [
        "###DeBERTa-large training\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1h2Cq1q_3HC6"
      },
      "outputs": [],
      "source": [
        "# empty the cache\n",
        "import torch \n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "kpkKoZfq7KE4",
        "outputId": "25a6505b-a441-482e-ca2c-d80523fb7343"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "You're using a DebertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='800' max='800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [800/800 22:37, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.730945</td>\n",
              "      <td>0.672792</td>\n",
              "      <td>0.710938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.786248</td>\n",
              "      <td>0.673869</td>\n",
              "      <td>0.718750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.912395</td>\n",
              "      <td>0.679321</td>\n",
              "      <td>0.712500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.600100</td>\n",
              "      <td>1.420408</td>\n",
              "      <td>0.671622</td>\n",
              "      <td>0.709375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.600100</td>\n",
              "      <td>1.649750</td>\n",
              "      <td>0.691464</td>\n",
              "      <td>0.723437</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=800, training_loss=0.41238888502120974, metrics={'train_runtime': 1359.9636, 'train_samples_per_second': 9.412, 'train_steps_per_second': 0.588, 'total_flos': 3476970671308800.0, 'train_loss': 0.41238888502120974, 'epoch': 5.0})"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# declare the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"output5\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    # push_to_hub=True,\n",
        ")\n",
        "\n",
        "#declare the trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_data[\"train\"],\n",
        "    eval_dataset=tokenized_data[\"val\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "#run the trainer\n",
        "trainer.train()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Cf4jqYg_3HC6"
      },
      "source": [
        "###DeBERTa-large inference\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "tnH8JRIc3HC6",
        "outputId": "014464f5-9cdf-4cfb-a8b6-c739e3682a0b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# load the trained model\n",
        "model_path = \"output5/checkpoint-800\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_path, num_labels=3, id2label=id2label, label2id=label2id\n",
        ").to(\"cuda\")\n",
        "\n",
        "# run the trainer and predict the outputs\n",
        "test_trainer = Trainer(model) \n",
        "\n",
        "raw_pred, labels, metrics = test_trainer.predict(tokenized_data[\"test\"]) \n",
        "\n",
        "y_pred = np.argmax(raw_pred, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1655DsA87nk",
        "outputId": "008089a3-f9e7-44b9-dbef-97247833716c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7276887505552079"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_f1 = f1_metric.compute(predictions=y_pred, references=labels, average=\"macro\")[\"f1\"]\n",
        "test_f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vh_GM7SK87nk",
        "outputId": "e6e5bd36-fd82-4dd8-9cf1-c86cb4efe69c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7325"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_accuracy = accuracy_metric.compute(predictions=y_pred, references=labels)[\"accuracy\"]\n",
        "test_accuracy"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "31WFbRSU6bhC"
      },
      "source": [
        "References:\n",
        "\n",
        "https://huggingface.co/microsoft/deberta-large\n",
        "\n",
        "https://huggingface.co/docs/transformers/tasks/sequence_classification\n",
        "\n",
        "\n",
        "> @inproceedings{\n",
        "he2021deberta,\\\n",
        "title={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},\\\n",
        "author={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},\\\n",
        "booktitle={International Conference on Learning Representations},\\\n",
        "year={2021},\\\n",
        "url={https://openreview.net/forum?id=XPZIaotutsD}\n",
        "}\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
