{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "t3IxC6FH3HC4"
      },
      "source": [
        "#DistilBERT base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZQaWfFsFaNT",
        "outputId": "a3f6aefd-3dc0-4fcc-b003-baaaf1dc7a0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting evaluate\n",
            "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0 (from transformers)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.7,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.4.0)\n",
            "Collecting aiohttp (from datasets)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting responses<0.19 (from datasets)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: tokenizers, xxhash, multidict, frozenlist, dill, async-timeout, yarl, responses, multiprocess, huggingface-hub, aiosignal, transformers, aiohttp, datasets, evaluate\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.12.0 dill-0.3.6 evaluate-0.4.0 frozenlist-1.3.3 huggingface-hub-0.14.1 multidict-6.0.4 multiprocess-0.70.14 responses-0.18.0 tokenizers-0.13.3 transformers-4.28.1 xxhash-3.2.0 yarl-1.9.2\n"
          ]
        }
      ],
      "source": [
        "#install transformers datasets evaluate\n",
        "!pip install transformers datasets evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87,
          "referenced_widgets": [
            "852d8f7b7e314e38b19729752dd9809a",
            "7ef2bc4eafd14d5fbda17f8d7afe122d",
            "a99191ae496e499f8612fac657d384c2",
            "35b0aa0387a14fe4b00dfc59447af139",
            "a3f35a23095140a0a8b473e3190e1ca3",
            "e6693bd77d694161962a0343ebdc0f40",
            "8ea4256e75a54222a279174294893f6d",
            "cdeee073d5ca40db8a3eb72b322d5a23",
            "0868f02c2ec3436aa196a09623b08655",
            "78c297ac70f84334b2e8ad022ac90552",
            "4d6c1d93b1404e5bb0227d12ca9d4cc0"
          ]
        },
        "id": "oqb9X5cE3HC5",
        "outputId": "476d1ce8-9e6b-4474-f030-93b3a36314fa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:datasets.builder:Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-a39e475095145fe4/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "852d8f7b7e314e38b19729752dd9809a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#import the dataset\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset('csv', data_files={'train': 'df_train.csv',\n",
        "                                          'val': 'df_valid.csv',\n",
        "                                              'test': 'df_test.csv'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "0437ed46debd4042a23438aaceb380fa",
            "a6eea0f99f6b473cb580c788ef13b46c",
            "dd24791fcdf041119d350b71e974cf73",
            "eda3c7bfd755494ca7a69d4a9332986e",
            "08064e6c15cf48fe82d1119d8d3286c0",
            "8445983a7f4d4d67996d13d39107e358",
            "d1f2264a0f714562810606541836b646",
            "986d6681d0284728b9d5b64d2a60fd60",
            "f3e0e086a9df4f499d40047ce6789217",
            "a90cdc9ba82d4647b1243c49ab7d390c",
            "61919494e86e4e5db1ea66080bfca4f7",
            "7a95a84573f34dbdb4754de7fcfafbbc",
            "a1eb87180c90474888c6e21cced1c919",
            "34ef1ee5112f42efb8787c7e8ee25a9a",
            "b13a78a2be7f4a90a81038a4406f3f51",
            "659132ffe8e343a0aabced676a401c09",
            "a60134125772431b983f6f4b74757b37",
            "34662376c841418aac5d92e27fda1130",
            "41d250ce2753449f9aba6cdf5f4fa1b8",
            "2f90cf26f03748f9977003921901f31e",
            "0e308ccb1c01402aa5f4f3c35d714893",
            "8f561e1bdd8b4a349153dbaeb264b35d",
            "66592a826d6f4880a477560d24e8d920",
            "26d059edbffe42b2ba498c06305e4e63",
            "46725e366ab4451f9a3eeb3cd2305f07",
            "96a2c43be0ce496884b71bb57625c965",
            "824cd601884f4f9380183f9b1e65b060",
            "7ae2e0d7e7334f5ab71a841b91ca5977",
            "5774926087ee45c0ac303ad465b2e34c",
            "3c073c0e1e73467cb6413836ef09ee15",
            "e82c4decc23a47dd9448c0880527452a",
            "7d5bca89610841c3a80a7aaa084bd72c",
            "4b67da357f2d40e29d4bac1488207256",
            "7eaa28b1aaf44ed8ad5f31aa3e273dc3",
            "5ab8a4a1657744af95d9749ab0e1584b",
            "19e65c552cf64c349ad9f0b70097d1b1",
            "8b97e62a25454bb7a7fc9e8361844872",
            "c02251baf90c408d99a31940e7269d6e",
            "edd8a51b92dd46b8aaa2286538b1af19",
            "f0cda62056124004b85293fd53630f30",
            "8d9286df7d4040149789c0cabe177bdb",
            "af5333b1561141bd92722ce6dc5df27c",
            "f0b9087839f242d5ae34bdceb31a14a4",
            "e65658a4944b46baade392de630b2dc2"
          ]
        },
        "id": "aMT-MrT3CCY7",
        "outputId": "6f5276c1-e5b9-4d29-86de-9b2da3190750"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0437ed46debd4042a23438aaceb380fa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7a95a84573f34dbdb4754de7fcfafbbc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/411 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "66592a826d6f4880a477560d24e8d920",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7eaa28b1aaf44ed8ad5f31aa3e273dc3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oV5ri_4l3HC5"
      },
      "outputs": [],
      "source": [
        "#preprocess the data by tokenizing the text\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples['text'], padding='max_length',\n",
        "                                max_length=128,\n",
        "                                truncation=True,\n",
        "                                return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVG-xFpD3HC5"
      },
      "outputs": [],
      "source": [
        "#declare dictionaries to map the labels\n",
        "id2label = {0: \"phrase\", 1: \"passage\", 2: \"multi\"}\n",
        "label2id = {\"phrase\": 0, \"passage\": 1, \"multi\": 2}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "446433c6edaa400e8668e29b566e53e3",
            "ca6cac9e47fd4b10a5b731b525571e54",
            "b98e06a4ae4442d18b4672179cf867e3",
            "a292cf2f9f4740cd93a7eac888cdb551",
            "2b47ffbc7bec410dbaeaa720d936570f",
            "b6efb91571814dd8937750affffefef0",
            "9cee302e829e47e2815c681957c959b0",
            "e03f0455d222430c95087f674ec6dfb8",
            "2d02c448f6444719addc80ae3b424cf9",
            "37ef1c937c0f4309914964740c3d8aee",
            "2164a4f6295f47df8cab25913eeb5a21"
          ]
        },
        "id": "nDrHd2w0CCY8",
        "outputId": "23967bb6-2719-45b0-dbc6-4039d08efb36"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "446433c6edaa400e8668e29b566e53e3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/263M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight']\n",
            "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"distilbert-base-cased\", num_labels=3, id2label=id2label, label2id=label2id\n",
        ").to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "2423e80c3fe949aa80da764e59589797",
            "4f42dab2b1cd4ba8be5b46f442400ebb",
            "0989c8806a7d4f4ba246c030e3e2837c",
            "f9641d12c5cc435198d0502a5448ac81",
            "6fd2bafda1464539962d17f847a59787",
            "11567ac6d0a7447a98ba85e8bb1ca4c3",
            "2252164ac80347ea92206ec16d6e719c",
            "281f5f4852114d64be6bdfe604b33177",
            "40a68232196e4f2fb0c06f5e0785ba49",
            "16d36e7e4d4d4391841a8859ae08325f",
            "8c4e9a2d637f4b86addb50e9487f3e05",
            "8fd327937a4a4c1e9631e615b26f4817",
            "811f1956f1fd4903a95396586d135f14",
            "18080b31c3ea4aa3b0bb4bc657691bdc",
            "0ba9b3338dbb4bf6bd50325b3132b36f",
            "604ed017db48485390c22abcbeb7c377",
            "21d88b53a6d14de3add79f430be6f661",
            "6ddb5d31a1ae4d139e234523763d9a2e",
            "0ad9911d5c8645dfa3b5ad8183c94d98",
            "cb5d1a66a7ec439abb849fd994391bbd",
            "c3a2d7c0a8c54ee4a0ff9febcddd1838",
            "4d9cf52757b24f06a04997ef6261ec4c",
            "3046c933096b45928649d3448929cc1b",
            "b51c008228614ecc93b461a4e3d09356",
            "e4b7acf28ef5493fb3b4fbbcfb190978",
            "461d618a4a5b46e691e728f1c82e894f",
            "2066eaabe4934baeb5be491c50a7703b",
            "090dde3c1a81426197b38a2ca1c79a66",
            "f888f1e175af4e0abbf582f82e6e7b89",
            "4a85f2fae84b4e7392c95f14727b76dd",
            "a5ce25fb627041a9bc30fc4763b3a6cb",
            "fbbd68c56b0345edbc5b5b158dc71803",
            "1bb263c53e3541d5a460cc52c4fc3eb5"
          ]
        },
        "id": "eYCA2Cp23HC5",
        "outputId": "42aff363-6fa5-449c-eadf-cb7d896077e4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2423e80c3fe949aa80da764e59589797",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/2560 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8fd327937a4a4c1e9631e615b26f4817",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/640 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3046c933096b45928649d3448929cc1b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#run the preprocess function on the input text\n",
        "tokenized_data = dataset.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "um4PgnHZ3HC5"
      },
      "outputs": [],
      "source": [
        "#import the metrics\n",
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "accuracy_metric = evaluate.load(\"accuracy\")\n",
        "f1_metric = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "\n",
        "  logits, labels = eval_pred\n",
        "  predictions = np.argmax(logits, axis=-1)\n",
        "  f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"macro\")[\"f1\"]\n",
        "  accuracy = accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
        "\n",
        "  return {\"f1\": f1, \"accuracy\": accuracy}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHKLu4QN3HC6"
      },
      "outputs": [],
      "source": [
        "#import the data collator of huggingface\n",
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jE6o5Xjq3HC6"
      },
      "source": [
        "###DeBERTa-large training\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1h2Cq1q_3HC6"
      },
      "outputs": [],
      "source": [
        "# empty the cache\n",
        "import torch \n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "WR3XJhFbFFVl",
        "outputId": "7f37baba-7991-497d-bba0-abff092ab9c8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='800' max='800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [800/800 02:53, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.939682</td>\n",
              "      <td>0.505188</td>\n",
              "      <td>0.553125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.847531</td>\n",
              "      <td>0.586386</td>\n",
              "      <td>0.625000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.854914</td>\n",
              "      <td>0.602520</td>\n",
              "      <td>0.642188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.848900</td>\n",
              "      <td>0.904398</td>\n",
              "      <td>0.603775</td>\n",
              "      <td>0.632812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.848900</td>\n",
              "      <td>0.938929</td>\n",
              "      <td>0.609635</td>\n",
              "      <td>0.642188</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=800, training_loss=0.695194091796875, metrics={'train_runtime': 173.8211, 'train_samples_per_second': 73.639, 'train_steps_per_second': 4.602, 'total_flos': 423903235276800.0, 'train_loss': 0.695194091796875, 'epoch': 5.0})"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# declare the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"output5\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    # push_to_hub=True,\n",
        ")\n",
        "\n",
        "#declare the trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_data[\"train\"],\n",
        "    eval_dataset=tokenized_data[\"val\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "#run the trainer\n",
        "trainer.train()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Cf4jqYg_3HC6"
      },
      "source": [
        "###DeBERTa-large inference\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "tnH8JRIc3HC6",
        "outputId": "014464f5-9cdf-4cfb-a8b6-c739e3682a0b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# load the trained model\n",
        "model_path = \"output5/checkpoint-800\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_path, num_labels=3, id2label=id2label, label2id=label2id\n",
        ").to(\"cuda\")\n",
        "\n",
        "# run the trainer and predict the outputs\n",
        "test_trainer = Trainer(model) \n",
        "\n",
        "raw_pred, labels, metrics = test_trainer.predict(tokenized_data[\"test\"]) \n",
        "\n",
        "y_pred = np.argmax(raw_pred, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jw4QCQ7qFFVm",
        "outputId": "b1376c1a-49ec-4460-8c1d-5249de8d0b6f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6136566019433247"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# compute f1 score of the model\n",
        "test_f1 = f1_metric.compute(predictions=y_pred, references=labels, average=\"macro\")[\"f1\"]\n",
        "test_f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQXTiamtFFVn",
        "outputId": "c444f095-741d-4bf1-b58a-bec28222ab36"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.62625"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# compute accuracy of the model\n",
        "\n",
        "test_accuracy = accuracy_metric.compute(predictions=y_pred, references=labels)[\"accuracy\"]\n",
        "test_accuracy"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "31WFbRSU6bhC"
      },
      "source": [
        "References:\n",
        "\n",
        "https://huggingface.co/distilbert-base-uncased\n",
        "\n",
        "https://huggingface.co/docs/transformers/tasks/sequence_classification\n",
        "\n",
        "\n",
        "> @article{Sanh2019DistilBERTAD,\\\n",
        "  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},\\\n",
        "  author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},\\\n",
        "  journal={ArXiv},\\\n",
        "  year={2019},\\\n",
        "  volume={abs/1910.01108}\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
